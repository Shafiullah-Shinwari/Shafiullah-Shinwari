{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYJ6IQA93/Xf+Oa8lP/GyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shafiullah-Shinwari/Shafiullah-Shinwari/blob/main/The_BlindTuner_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "9UOPdai_kT8H",
        "outputId": "6331884d-6b90-40b6-c2cf-0cc0386e0a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'V' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-aa58e08c08a4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# Run BlindTuner system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mfinal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblind_tuner_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-aa58e08c08a4>\u001b[0m in \u001b[0;36mblind_tuner_system\u001b[0;34m(Xtrain, Ytrain, Xval, Yval, public_key, private_key, alpha, batch_size, gamma, epochs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Step 6a: Train with encrypted data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Step 6b: Validate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-aa58e08c08a4>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, cloud_data, optimizer, alpha, batch_size, gamma, public_key, private_key)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnag_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Step 6c: Send logits to the client (decryption happens here)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'V' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from timm import create_model  # DEiT model (from timm library)\n",
        "import numpy as np\n",
        "\n",
        "# Helper function to simulate NAG update\n",
        "def nag_update(W, V, grad, alpha, batch_size, gamma):\n",
        "    \"\"\"\n",
        "    Perform a Nesterov Accelerated Gradient update.\n",
        "    \"\"\"\n",
        "    V_next = (1 - gamma) * W + gamma * V  # Momentum update\n",
        "    W_next = V_next - (alpha / batch_size) * grad  # Gradient step\n",
        "    return W_next, V_next\n",
        "\n",
        "# Dummy encryption/decryption functions (replace with actual FHE)\n",
        "def encrypt_data(X, Y, public_key):\n",
        "    # Simulate encryption (replace with actual encryption)\n",
        "    return X, Y  # Placeholder for encrypted data\n",
        "\n",
        "def decrypt_data(logits, private_key):\n",
        "    # Simulate decryption (replace with actual decryption)\n",
        "    return logits  # Placeholder for decrypted logits\n",
        "\n",
        "def train_step(model, cloud_data, optimizer, alpha, batch_size, gamma, public_key, private_key):\n",
        "    \"\"\"\n",
        "    A single training step that mimics the BlindTuner process.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Step 6b: Update parameters using NAG\n",
        "    for Xct, Yct in cloud_data:\n",
        "        # Encrypt the data (Xct, Yct)\n",
        "        Xct_enc, Yct_enc = encrypt_data(Xct, Yct, public_key)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(Xct_enc)\n",
        "        loss = F.cross_entropy(logits, Yct_enc)  # Compute loss\n",
        "\n",
        "        # Compute gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights with NAG\n",
        "        for param in model.parameters():\n",
        "            if param.grad is not None:\n",
        "                grad = param.grad\n",
        "                param.data, V = nag_update(W=param.data, V=V, grad=grad, alpha=alpha, batch_size=batch_size, gamma=gamma)\n",
        "\n",
        "        # Step 6c: Send logits to the client (decryption happens here)\n",
        "        logits_dec = decrypt_data(logits, private_key)\n",
        "        return logits_dec\n",
        "\n",
        "def validate_step(model, val_data, public_key, private_key):\n",
        "    \"\"\"\n",
        "    Simulate validation step where the logits are decrypted and loss is computed.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Decrypt logits and calculate validation loss\n",
        "    val_logits = model(val_data)\n",
        "    val_loss = F.cross_entropy(val_logits, Yct_val)\n",
        "    return val_loss\n",
        "\n",
        "def blind_tuner_system(Xtrain, Ytrain, Xval, Yval, public_key, private_key, alpha, batch_size, gamma, epochs):\n",
        "    # Step 1: Feature Extraction using DEiT\n",
        "    model = create_model('deit_base_distilled_patch16_224', pretrained=True)\n",
        "\n",
        "    # Step 2: Simulate encryption\n",
        "    encrypted_Xtrain, encrypted_Ytrain = encrypt_data(Xtrain, Ytrain, public_key)\n",
        "    encrypted_Xval, encrypted_Yval = encrypt_data(Xval, Yval, public_key)\n",
        "\n",
        "    # Prepare DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(list(zip(encrypted_Xtrain, encrypted_Ytrain)), batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(list(zip(encrypted_Xval, encrypted_Yval)), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Step 3: Initialize optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=alpha, momentum=0.9)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        # Step 6a: Train with encrypted data\n",
        "        train_step(model, train_loader, optimizer, alpha, batch_size, gamma, public_key, private_key)\n",
        "\n",
        "        # Step 6b: Validate the model\n",
        "        val_loss = validate_step(model, val_loader, public_key, private_key)\n",
        "        print(f\"Validation Loss: {val_loss.item()}\")\n",
        "\n",
        "    # Final model transmission (Step 4)\n",
        "    # Decrypt the model weights to obtain the final model\n",
        "    Wpt = decrypt_data(model.state_dict(), private_key)\n",
        "    return Wpt\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Example dataset (replace with actual dataset and keys)\n",
        "    Xtrain, Ytrain = torch.randn(100, 3, 224, 224), torch.randint(0, 1000, (100,))\n",
        "    Xval, Yval = torch.randn(20, 3, 224, 224), torch.randint(0, 1000, (20,))\n",
        "\n",
        "    # Placeholder encryption keys (you'll need actual keys from an encryption library)\n",
        "    public_key = None\n",
        "    private_key = None\n",
        "\n",
        "    # Hyperparameters\n",
        "    alpha = 0.01\n",
        "    batch_size = 32\n",
        "    gamma = 0.9\n",
        "    epochs = 10\n",
        "\n",
        "    # Run BlindTuner system\n",
        "    final_model = blind_tuner_system(Xtrain, Ytrain, Xval, Yval, public_key, private_key, alpha, batch_size, gamma, epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EyCmJZP8NWH-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vtAwyPhkVwOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from timm import create_model  # DEiT model (from timm library)\n",
        "import numpy as np\n",
        "\n",
        "# ... (encrypt_data, decrypt_data functions remain the same)\n",
        "\n",
        "def nag_update(W, V, grad, alpha, batch_size, gamma):\n",
        "    \"\"\"\n",
        "    Perform a Nesterov Accelerated Gradient update.\n",
        "    \"\"\"\n",
        "    V_next = (1 - gamma) * W + gamma * V  # Momentum update\n",
        "    W_next = V_next - (alpha / batch_size) * grad  # Gradient step\n",
        "    return W_next, V_next\n",
        "\n",
        "\n",
        "def train_step(model, cloud_data, optimizer, alpha, batch_size, gamma, public_key, private_key):\n",
        "    \"\"\"\n",
        "    A single training step that mimics the BlindTuner process.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    # Initialize V for each parameter\n",
        "    V_dict = {name: torch.zeros_like(param) for name, param in model.state_dict().items()}\n",
        "\n",
        "    for Xct, Yct in cloud_data:\n",
        "        # Encrypt the data (Xct, Yct)\n",
        "        Xct_enc, Yct_enc = encrypt_data(Xct, Yct, public_key)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(Xct_enc)\n",
        "        loss = F.cross_entropy(logits, Yct_enc)  # Compute loss\n",
        "\n",
        "        # Compute gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights with NAG\n",
        "        for name, param in model.parameters():\n",
        "            if param.grad is not None:\n",
        "                grad = param.grad\n",
        "                # Get V for the current parameter\n",
        "                V = V_dict[name]\n",
        "                # Update parameter and V using nag_update\n",
        "                param.data, V_dict[name] = nag_update(W=param.data, V=V, grad=grad, alpha=alpha, batch_size=batch_size, gamma=gamma)\n",
        "\n",
        "        # Step 6c: Send logits to the client (decryption happens here)\n",
        "        logits_dec = decrypt_data(logits, private_key)\n",
        "        return logits_dec\n",
        "\n",
        "def validate_step(model, val_data, public_key, private_key):\n",
        "    \"\"\"\n",
        "    Simulate validation step where the logits are decrypted and loss is computed.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Decrypt logits and calculate validation loss\n",
        "    val_logits = model(val_data)\n",
        "    val_loss = F.cross_entropy(val_logits, Yct_val)\n",
        "    return val_loss\n",
        "\n",
        "def blind_tuner_system(Xtrain, Ytrain, Xval, Yval, public_key, private_key, alpha, batch_size, gamma, epochs):\n",
        "    # Step 1: Feature Extraction using DEiT\n",
        "    model = create_model('deit_base_distilled_patch16_224', pretrained=True)\n",
        "\n",
        "    # Step 2: Simulate encryption\n",
        "    encrypted_Xtrain, encrypted_Ytrain = encrypt_data(Xtrain, Ytrain, public_key)\n",
        "    encrypted_Xval, encrypted_Yval = encrypt_data(Xval, Yval, public_key)\n",
        "\n",
        "    # Prepare DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(list(zip(encrypted_Xtrain, encrypted_Ytrain)), batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(list(zip(encrypted_Xval, encrypted_Yval)), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Step 3: Initialize optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=alpha, momentum=0.9)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        # Step 6a: Train with encrypted data\n",
        "        train_step(model, train_loader, optimizer, alpha, batch_size, gamma, public_key, private_key)\n",
        "\n",
        "        # Step 6b: Validate the model\n",
        "        val_loss = validate_step(model, val_loader, public_key, private_key)\n",
        "        print(f\"Validation Loss: {val_loss.item()}\")\n",
        "\n",
        "    # Final model transmission (Step 4)\n",
        "    # Decrypt the model weights to obtain the final model\n",
        "    Wpt = decrypt_data(model.state_dict(), private_key)\n",
        "    return Wpt\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Example dataset (replace with actual dataset and keys)\n",
        "    Xtrain, Ytrain = torch.randn(100, 3, 224, 224), torch.randint(0, 1000, (100,))\n",
        "    Xval, Yval = torch.randn(20, 3, 224, 224), torch.randint(0, 1000, (20,))\n",
        "\n",
        "    # Placeholder encryption keys (you'll need actual keys from an encryption library)\n",
        "    public_key = None\n",
        "    private_key = None\n",
        "\n",
        "    # Hyperparameters\n",
        "    alpha = 0.01\n",
        "    batch_size = 32\n",
        "    gamma = 0.9\n",
        "    epochs = 10\n",
        "\n",
        "    # Run BlindTuner system\n",
        "    final_model = blind_tuner_system(Xtrain, Ytrain, Xval, Yval, public_key, private_key, alpha, batch_size, gamma, epochs)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "TNZU90Z_XUA6",
        "outputId": "a49e9ded-02f8-4e69-ed58-4bcdc24f8c8b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-eabc695281b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Run BlindTuner system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mfinal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblind_tuner_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-eabc695281b9>\u001b[0m in \u001b[0;36mblind_tuner_system\u001b[0;34m(Xtrain, Ytrain, Xval, Yval, public_key, private_key, alpha, batch_size, gamma, epochs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Step 6a: Train with encrypted data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Step 6b: Validate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-eabc695281b9>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, cloud_data, optimizer, alpha, batch_size, gamma, public_key, private_key)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Update weights with NAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gyUQWhToYZWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from timm import create_model\n",
        "import numpy as np\n",
        "\n",
        "# ... (encrypt_data, decrypt_data functions remain the same)\n",
        "\n",
        "def nag_update(W, V, grad, alpha, batch_size, gamma):\n",
        "    \"\"\"\n",
        "    Perform a Nesterov Accelerated Gradient update.\n",
        "    \"\"\"\n",
        "    V_next = (1 - gamma) * W + gamma * V\n",
        "    W_next = V_next - (alpha / batch_size) * grad\n",
        "    return W_next, V_next\n",
        "\n",
        "def train_step(model, cloud_data, optimizer, alpha, batch_size, gamma, public_key, private_key):\n",
        "    \"\"\"\n",
        "    A single training step that mimics the BlindTuner process.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    V_dict = {name: torch.zeros_like(param) for name, param in model.state_dict().items()}\n",
        "\n",
        "    for Xct, Yct in cloud_data:\n",
        "        Xct_enc, Yct_enc = encrypt_data(Xct, Yct, public_key)\n",
        "        logits = model(Xct_enc)\n",
        "        loss = F.cross_entropy(logits, Yct_enc)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights with NAG (using model.named_parameters())\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                grad = param.grad\n",
        "                V = V_dict[name]\n",
        "                param.data, V_dict[name] = nag_update(W=param.data, V=V, grad=grad, alpha=alpha, batch_size=batch_size, gamma=gamma)\n",
        "\n",
        "        logits_dec = decrypt_data(logits, private_key)\n",
        "        return logits_dec\n",
        "\n",
        "def validate_step(model, val_data, Yct_val, public_key, private_key): # Added Yct_val as an argument\n",
        "    \"\"\"\n",
        "    Simulate validation step where the logits are decrypted and loss is computed.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_logits = model(val_data)\n",
        "\n",
        "    # Calculate validation loss\n",
        "    val_loss = F.cross_entropy(val_logits, Yct_val)\n",
        "    return val_loss\n",
        "\n",
        "def blind_tuner_system(Xtrain, Ytrain, Xval, Yval, public_key, private_key, alpha, batch_size, gamma, epochs):\n",
        "    # ... (rest of the code remains the same)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        train_step(model, train_loader, optimizer, alpha, batch_size, gamma, public_key, private_key)\n",
        "\n",
        "        # Pass encrypted Yval to validate_step\n",
        "        val_loss = validate_step(model, encrypted_Xval, encrypted_Yval, public_key, private_key)\n",
        "        print(f\"Validation Loss: {val_loss.item()}\")\n",
        "\n",
        "    # ... (rest of the code remains the same)\n",
        "\n",
        "# ... (example usage remains the same)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0Lfo4xSfYbPy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from timm import create_model\n",
        "import numpy as np\n",
        "\n",
        "# Dummy encryption/decryption functions (replace with actual FHE)\n",
        "def encrypt_data(X, Y, public_key):\n",
        "    # Simulate encryption (replace with actual encryption)\n",
        "    return X, Y  # Placeholder for encrypted data\n",
        "\n",
        "def decrypt_data(logits, private_key):\n",
        "    # Simulate decryption (replace with actual decryption)\n",
        "    return logits  # Placeholder for decrypted logits\n",
        "\n",
        "def nag_update(W, V, grad, alpha, batch_size, gamma):\n",
        "    \"\"\"\n",
        "    Perform a Nesterov Accelerated Gradient update.\n",
        "    \"\"\"\n",
        "    V_next = (1 - gamma) * W + gamma * V\n",
        "    W_next = V_next - (alpha / batch_size) * grad\n",
        "    return W_next, V_next\n",
        "\n",
        "def train_step(model, cloud_data, optimizer, alpha, batch_size, gamma, public_key, private_key):\n",
        "    \"\"\"\n",
        "    A single training step that mimics the BlindTuner process.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    V_dict = {name: torch.zeros_like(param) for name, param in model.state_dict().items()}\n",
        "\n",
        "    for Xct, Yct in cloud_data:\n",
        "        Xct_enc, Yct_enc = encrypt_data(Xct, Yct, public_key)\n",
        "        logits = model(Xct_enc)\n",
        "        loss = F.cross_entropy(logits, Yct_enc)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights with NAG (using model.named_parameters())\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                grad = param.grad\n",
        "                V = V_dict[name]\n",
        "                param.data, V_dict[name] = nag_update(W=param.data, V=V, grad=grad, alpha=alpha, batch_size=batch_size, gamma=gamma)\n",
        "\n",
        "        logits_dec = decrypt_data(logits, private_key)\n",
        "        return logits_dec\n",
        "\n",
        "def validate_step(model, val_data, Yct_val, public_key, private_key):\n",
        "    \"\"\"\n",
        "    Simulate validation step where the logits are decrypted and loss is computed.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_logits = model(val_data)\n",
        "\n",
        "    # Calculate validation loss\n",
        "    val_loss = F.cross_entropy(val_logits, Yct_val)\n",
        "    return val_loss\n",
        "\n",
        "def blind_tuner_system(Xtrain, Ytrain, Xval, Yval, public_key, private_key, alpha, batch_size, gamma, epochs):\n",
        "    # Step 1: Feature Extraction using DEiT\n",
        "    model = create_model('deit_base_distilled_patch16_224', pretrained=True)\n",
        "\n",
        "    # Step 2: Simulate encryption\n",
        "    encrypted_Xtrain, encrypted_Ytrain = encrypt_data(Xtrain, Ytrain, public_key)\n",
        "    encrypted_Xval, encrypted_Yval = encrypt_data(Xval, Yval, public_key)\n",
        "\n",
        "    # Prepare DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(list(zip(encrypted_Xtrain, encrypted_Ytrain)), batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(list(zip(encrypted_Xval, encrypted_Yval)), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Step 3: Initialize optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=alpha, momentum=0.9)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        train_step(model, train_loader, optimizer, alpha, batch_size, gamma, public_key, private_key)\n",
        "\n",
        "        # Pass encrypted Yval to validate_step\n",
        "        val_loss = validate_step(model, encrypted_Xval, encrypted_Yval, public_key, private_key)\n",
        "        print(f\"Validation Loss: {val_loss.item()}\")\n",
        "\n",
        "    # Final model transmission (Step 4)\n",
        "    # Decrypt the model weights to obtain the final model\n",
        "    Wpt = decrypt_data(model.state_dict(), private_key)\n",
        "    return Wpt\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Example dataset (replace with actual dataset and keys)\n",
        "    Xtrain, Ytrain = torch.randn(100, 3, 224, 224), torch.randint(0, 1000, (100,))\n",
        "    Xval, Yval = torch.randn(20, 3, 224, 224), torch.randint(0, 1000, (20,))\n",
        "\n",
        "    # Placeholder encryption keys (you'll need actual keys from an encryption library)\n",
        "    public_key = None\n",
        "    private_key = None\n",
        "\n",
        "    # Hyperparameters\n",
        "    alpha = 0.01\n",
        "    batch_size = 32\n",
        "    gamma = 0.9\n",
        "    epochs = 10\n",
        "\n",
        "    # Run BlindTuner system\n",
        "    final_model = blind_tuner_system(Xtrain, Ytrain, Xval, Yval, public_key, private_key, alpha, batch_size, gamma, epochs)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ulM13Q3ZFjd",
        "outputId": "33a40c09-dbed-44f6-9d18-1d5a1638ef32"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Validation Loss: 6.897387504577637\n",
            "Epoch 2/10\n",
            "Validation Loss: 6.906926155090332\n",
            "Epoch 3/10\n",
            "Validation Loss: 6.9076714515686035\n",
            "Epoch 4/10\n",
            "Validation Loss: 6.9077467918396\n",
            "Epoch 5/10\n",
            "Validation Loss: 6.907755374908447\n",
            "Epoch 6/10\n",
            "Validation Loss: 6.907753944396973\n",
            "Epoch 7/10\n",
            "Validation Loss: 6.907753944396973\n",
            "Epoch 8/10\n",
            "Validation Loss: 6.907753944396973\n",
            "Epoch 9/10\n",
            "Validation Loss: 6.907755374908447\n",
            "Epoch 10/10\n",
            "Validation Loss: 6.907755374908447\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNKDCwAv3rrUQK/JVyTAfg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shafiullah-Shinwari/Shafiullah-Shinwari/blob/main/The_BlindTuner_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EyCmJZP8NWH-"
      }
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from timm import create_model\n",
        "import numpy as np\n",
        "\n",
        "# Dummy encryption/decryption functions (replace with actual FHE)\n",
        "def encrypt_data(X, Y, public_key):\n",
        "    # Simulate encryption (replace with actual encryption)\n",
        "    return X, Y  # Placeholder for encrypted data\n",
        "\n",
        "def decrypt_data(logits, private_key):\n",
        "    # Simulate decryption (replace with actual decryption)\n",
        "    return logits  # Placeholder for decrypted logits\n",
        "\n",
        "def nag_update(W, V, grad, alpha, batch_size, gamma):\n",
        "    \"\"\"\n",
        "    Perform a Nesterov Accelerated Gradient update.\n",
        "    \"\"\"\n",
        "    V_next = (1 - gamma) * W + gamma * V\n",
        "    W_next = V_next - (alpha / batch_size) * grad\n",
        "    return W_next, V_next\n",
        "\n",
        "def train_step(model, cloud_data, optimizer, alpha, batch_size, gamma, public_key, private_key):\n",
        "    \"\"\"\n",
        "    A single training step that mimics the BlindTuner process.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    V_dict = {name: torch.zeros_like(param) for name, param in model.state_dict().items()}\n",
        "\n",
        "    for Xct, Yct in cloud_data:\n",
        "        Xct_enc, Yct_enc = encrypt_data(Xct, Yct, public_key)\n",
        "        logits = model(Xct_enc)\n",
        "        loss = F.cross_entropy(logits, Yct_enc)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights with NAG (using model.named_parameters())\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                grad = param.grad\n",
        "                V = V_dict[name]\n",
        "                param.data, V_dict[name] = nag_update(W=param.data, V=V, grad=grad, alpha=alpha, batch_size=batch_size, gamma=gamma)\n",
        "\n",
        "        logits_dec = decrypt_data(logits, private_key)\n",
        "        return logits_dec\n",
        "\n",
        "def validate_step(model, val_data, Yct_val, public_key, private_key):\n",
        "    \"\"\"\n",
        "    Simulate validation step where the logits are decrypted and loss is computed.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_logits = model(val_data)\n",
        "\n",
        "    # Calculate validation loss\n",
        "    val_loss = F.cross_entropy(val_logits, Yct_val)\n",
        "    return val_loss\n",
        "\n",
        "def blind_tuner_system(Xtrain, Ytrain, Xval, Yval, public_key, private_key, alpha, batch_size, gamma, epochs):\n",
        "    # Step 1: Feature Extraction using DEiT\n",
        "    model = create_model('deit_base_distilled_patch16_224', pretrained=True)\n",
        "\n",
        "    # Step 2: Simulate encryption\n",
        "    encrypted_Xtrain, encrypted_Ytrain = encrypt_data(Xtrain, Ytrain, public_key)\n",
        "    encrypted_Xval, encrypted_Yval = encrypt_data(Xval, Yval, public_key)\n",
        "\n",
        "    # Prepare DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(list(zip(encrypted_Xtrain, encrypted_Ytrain)), batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(list(zip(encrypted_Xval, encrypted_Yval)), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Step 3: Initialize optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=alpha, momentum=0.9)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        train_step(model, train_loader, optimizer, alpha, batch_size, gamma, public_key, private_key)\n",
        "\n",
        "        # Pass encrypted Yval to validate_step\n",
        "        val_loss = validate_step(model, encrypted_Xval, encrypted_Yval, public_key, private_key)\n",
        "        print(f\"Validation Loss: {val_loss.item()}\")\n",
        "\n",
        "    # Final model transmission (Step 4)\n",
        "    # Decrypt the model weights to obtain the final model\n",
        "    Wpt = decrypt_data(model.state_dict(), private_key)\n",
        "    return Wpt\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Example dataset (replace with actual dataset and keys)\n",
        "    Xtrain, Ytrain = torch.randn(100, 3, 224, 224), torch.randint(0, 1000, (100,))\n",
        "    Xval, Yval = torch.randn(20, 3, 224, 224), torch.randint(0, 1000, (20,))\n",
        "\n",
        "    # Placeholder encryption keys (you'll need actual keys from an encryption library)\n",
        "    public_key = None\n",
        "    private_key = None\n",
        "\n",
        "    # Hyperparameters\n",
        "    alpha = 0.01\n",
        "    batch_size = 32\n",
        "    gamma = 0.9\n",
        "    epochs = 10\n",
        "\n",
        "    # Run BlindTuner system\n",
        "    final_model = blind_tuner_system(Xtrain, Ytrain, Xval, Yval, public_key, private_key, alpha, batch_size, gamma, epochs)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ulM13Q3ZFjd",
        "outputId": "33a40c09-dbed-44f6-9d18-1d5a1638ef32"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Validation Loss: 6.897387504577637\n",
            "Epoch 2/10\n",
            "Validation Loss: 6.906926155090332\n",
            "Epoch 3/10\n",
            "Validation Loss: 6.9076714515686035\n",
            "Epoch 4/10\n",
            "Validation Loss: 6.9077467918396\n",
            "Epoch 5/10\n",
            "Validation Loss: 6.907755374908447\n",
            "Epoch 6/10\n",
            "Validation Loss: 6.907753944396973\n",
            "Epoch 7/10\n",
            "Validation Loss: 6.907753944396973\n",
            "Epoch 8/10\n",
            "Validation Loss: 6.907753944396973\n",
            "Epoch 9/10\n",
            "Validation Loss: 6.907755374908447\n",
            "Epoch 10/10\n",
            "Validation Loss: 6.907755374908447\n"
          ]
        }
      ]
    }
  ]
}